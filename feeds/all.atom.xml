<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Geert's Notes</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2019-12-05T19:30:00+01:00</updated><entry><title>DNS</title><link href="/dns.html" rel="alternate"></link><published>2019-10-28T00:00:00+01:00</published><updated>2019-10-28T00:00:00+01:00</updated><author><name>Geert Jongen</name></author><id>tag:None,2019-10-28:/dns.html</id><summary type="html">&lt;h2&gt;DNS&lt;/h2&gt;
&lt;p&gt;This article is about DNS. Usually I do not have to worry about this but as you
start running apps in the cloud you have to understand at least the basics.
That is my goal with this page. How can you use DNS and what should you understand
when …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;DNS&lt;/h2&gt;
&lt;p&gt;This article is about DNS. Usually I do not have to worry about this but as you
start running apps in the cloud you have to understand at least the basics.
That is my goal with this page. How can you use DNS and what should you understand
when you quickly try to put apps in production using a cloud service provider.&lt;/p&gt;
&lt;h2&gt;DNS&lt;/h2&gt;
&lt;p&gt;In a nutshell, DNS uses socalled nameservers to find where the content is stored that you are looking for
you usually start at top-level-domains. There are two types countrycode TLD (i.e., .nl), and gTLD (.com). Globally there are 13
root servers hosted by IANA that contain addresses to DNS servers for these TLDs.&lt;/p&gt;
&lt;p&gt;For example, the root server points to the country authoritative server for .nl 
which is managed by (https://www.sidn.nl/)[SIDN]. Further (https://public-dns.info/nameserver/nl.html)[here] is a list with all public DNS servers in the Netherlands. 
Most of these are unauthoriatitve and therefore cache information from authorative servers.
This is also the reason that an update to your dns info might take 48hrs before it is
fully propagated across all DNS nameservers.&lt;/p&gt;
&lt;p&gt;Next as an organization you would get your domain by setting it up through a registrar.
this organization has the delegated responsiblity within the country-specific domain to set up
local zones. I might for example registrate datascience.nl. This then means that within
datascience.nl i can create subdomains that i point to.&lt;/p&gt;
&lt;h2&gt;DNS Record&lt;/h2&gt;
&lt;p&gt;There are basically two types of DNS records. 
1. An Arecord which maps a (sub)domain to an IP address www.datascience.nl &amp;gt; 192.168.1.1
2. CNAME which maps a (sub)domain to another domain my.datascience.nl &amp;gt; www.datascience.nl
Also seperately
3. MXNET for email which maps to a mail server&lt;/p&gt;
&lt;p&gt;When there is a CNAME, the DNS server will not return the domain, but visit this domain.
This will continue until you hit an ARECORD that will return an IP. The chain of CNAMEs is not limited
in any way.&lt;/p&gt;
&lt;h2&gt;Dig&lt;/h2&gt;
&lt;p&gt;dig is a linux tool for querying DNS servers. See &lt;a href="here"&gt;https://www.madboa.com/geek/dig/#understanding-the-default-output&lt;/a&gt; a
detailed howto. It can help in understanding what a DNS server is doing.&lt;/p&gt;
&lt;h3&gt;issues&lt;/h3&gt;
&lt;p&gt;One issue i came accross was that if you map a CNAME to the root domain (datascience.nl) 
it will assume there are no other records. Hence, any subdomains or mail will not be picked up. see more details
&lt;a href="here"&gt;https://www.freecodecamp.org/news/why-cant-a-domain-s-root-be-a-cname-8cbab38e5f5c/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The problem arises as you want to point your naked domain to something like a load balancer from AWS.
The ips of these ELBs are not static and might change. Hence, AWS requires you to use the URL. This can only
be done through a CNAME. Any subdomains [Arecords] or mail [MXNET] records will be ignored.&lt;/p&gt;
&lt;p&gt;To overcome this issue some DNS providers have provided something called CNAME flattening. Basically 
it provides a CNAME implementation without any downsides. Unfortanetly this is not standard, so different
DNS providers provide different implementations or none at all.&lt;/p&gt;
&lt;p&gt;Solutions are
&lt;code&gt;ALIAS&lt;/code&gt; at DNSimple
&lt;code&gt;ANAME&lt;/code&gt; at DNS made easy
&lt;code&gt;ANAME&lt;/code&gt; at easyDNS
&lt;code&gt;CNAME&lt;/code&gt;(virtual) at Cloudflare&lt;/p&gt;</content></entry><entry><title>Spark AI Summit 2019 Notes</title><link href="/spark-ai-summit-2019-notes.html" rel="alternate"></link><published>2019-10-24T12:00:00+02:00</published><updated>2019-12-05T19:30:00+01:00</updated><author><name>Geert Jongen</name></author><id>tag:None,2019-10-24:/spark-ai-summit-2019-notes.html</id><summary type="html">&lt;p&gt;Short version for index and feeds&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Spark AI Summit&lt;/h1&gt;
&lt;p&gt;Last week i attended the Spark+AI summit in Amsterdam. This was the first time I went to this conference 
and it showed my lots of new features and ideas that I am now quite excited about. In this post I would like to 
share some interesting ideas and tools that I came across, and that I will definitely investigate further.&lt;/p&gt;
&lt;h2&gt;MlFlow&lt;/h2&gt;
&lt;p&gt;One of the most interesting tools was definitely MlFlow. This tool fills one of the big gaps that I see in 
practice when building data science models. Usually you start with a model and then slowly start building more 
features and hence better performance. However, quite quickly you get marginal increases in performance and 
you start to think about the complexity/performance trade-off. At that point a tool like MlFLow is really 
helpful in that automatically logs each run of your model, while also logging the git commit of the code. &lt;/p&gt;
&lt;p&gt;In addition, it also saves the model in pickled format so that you can easily export it to a production environment when 
you are ready. I have already played with this the last weekend and got some interesting results. What I have not solved 
however, is how to store the data pipeline code and version together with the model code. 
In order to take an even higher-level point of view I will start to investigate Kubeflow  and Hopsworks which were both mentioned in several talks.&lt;/p&gt;
&lt;h3&gt;Other Topics&lt;/h3&gt;
&lt;p&gt;In addition a lot of talks mentioned that data quality issues are by far the biggest issue bottlenecking data science performance. Interesting new tools &amp;amp; packages are:
1.  Of course delta from databricks which provides an ACID data lake to Spark and big data. It sounds very exciting but I could not determine up to what extent all the features are open source or only included in the commercial version from Databricks. (https://delta.io/). As an alternative Iceberg was mentioned. Where Delta is more column based, Iceberg is the rowbased implementation. 
2.   Apache kafka is used virtually every presentation showing how their IT infrastructure glues all components together
3.  Debezium an interesting concept where you stream schema changes and updates using the logs of the source database to your feature engineering code. Auto-adjust based on schema changes for example.
4.  Zipline, airbnb’s implementation for Feature engineering taking into account streams of data where it can be hard to know what each variable was at each point in time, and how to time travel back and forth.
5.  Waimak, a framework to help you build spark pipelines faster &lt;/p&gt;</content><category term="spark"></category><category term="summit"></category><category term="notes"></category><category term="mlflow"></category></entry><entry><title>MlFlow Lessons Learned</title><link href="/mlflow-lessons-learned.html" rel="alternate"></link><published>2019-10-24T00:00:00+02:00</published><updated>2019-10-24T00:00:00+02:00</updated><author><name>Geert Jongen</name></author><id>tag:None,2019-10-24:/mlflow-lessons-learned.html</id><summary type="html">&lt;h1&gt;MlFlow&lt;/h1&gt;
&lt;p&gt;As shown in the post about my Spark+AI summit notes, I got interested to 
test out the performance of mlflow.&lt;/p&gt;
&lt;h2&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;The quickstart on the mlflow website is very simple to follow and will set
you up quickly. One thing i missed is that if you run in …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;MlFlow&lt;/h1&gt;
&lt;p&gt;As shown in the post about my Spark+AI summit notes, I got interested to 
test out the performance of mlflow.&lt;/p&gt;
&lt;h2&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;The quickstart on the mlflow website is very simple to follow and will set
you up quickly. One thing i missed is that if you run in local mode, the
results will be written away inside a .mlflow folder in your projects code.
Hence, there is no need to create a centralized place to store your runs. I guess
this makes sense when developing on your local machine.&lt;/p&gt;
&lt;h2&gt;Cloud Setup&lt;/h2&gt;
&lt;p&gt;Install on AWS
It is possible to install the mlflow application on AWS. The goal is to create a 
central app that can be used during a workshop to compare various models from different
teams. In this way we can basically duplicate the kaggle competition
layout.
1. People send in their runs from which we can see what the performance is on the 
train data
2. we register the model and run it ourselves on a hidden test dataset and put the
scores on a website&lt;/p&gt;</content><category term="mlflow"></category><category term="aws"></category></entry></feed>