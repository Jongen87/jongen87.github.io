<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Geert's Notes - AWS S3</title><link href="/" rel="alternate"></link><link href="/feeds/aws-s3.atom.xml" rel="self"></link><id>/</id><updated>2019-12-03T19:30:00+01:00</updated><entry><title>Using AWS S3 in a Workshop as Submissions Source</title><link href="/aws-s3-workshop.html" rel="alternate"></link><published>2019-12-03T12:00:00+01:00</published><updated>2019-12-03T19:30:00+01:00</updated><author><name>Geert Jongen</name></author><id>tag:None,2019-12-03:/aws-s3-workshop.html</id><summary type="html">&lt;p&gt;This post contains my notes on AWS S3 when we implemented it to store submissions for a workshop in Google Colab&lt;/p&gt;</summary><content type="html">&lt;h1&gt;S3 in a workshop&lt;/h1&gt;
&lt;p&gt;Last week we organized a Keras/Tensorflow workshop for pydata participants.
The goal was to have them run a jupyter notebook using Google Colab. The resulting model would then be uploaded to an S3 bucket
so that we could rate the models in a small challenge. Using S3 we would then be able to download these models onto an environment
where the 'secret' testt data set was present and we could quickly verify that the neural net perfomed well on new data.&lt;/p&gt;
&lt;h2&gt;Attempt 1: Using Presigned Links&lt;/h2&gt;
&lt;p&gt;Using the python library &lt;code&gt;boto3&lt;/code&gt; i knew that it was possible to create a pre-signed URL. Using this URL we would be able to
include temporary credentials within the link. Hence, we could keep the bucket private, while giving write-only access to users
to store their best model within the S3 bucket. In addition, such a link includes a timestamp up to which it is valid. This would
automatically enforce the deadline for the challenge.&lt;/p&gt;
&lt;p&gt;In order to generate the url i used the following code from boto3:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;instantiate your client &lt;code&gt;s3_client = boto3.client('s3', region_name='eu-west-2')&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use the boto built-in function &lt;code&gt;generate_presigned_post()&lt;/code&gt; where bucket_name is the bucket you want to use, object_name is the
object to which it will be uploaded &lt;code&gt;folder/folder/filename&lt;/code&gt;, conditions is a dict containing constraints and expiration is a number
in seconds from moment of generation during which this link can be used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s3_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate_presigned_post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bucket_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;object_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;conditions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conditions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;expiresin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expiration&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;next you can upload a file using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;file.hd5&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;file&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;file.hd5&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
        &lt;span class="n"&gt;http_response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fields&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# If successful, returns HTTP status code 204&lt;/span&gt;
    &lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;File upload HTTP status code: {http_response.status_code}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Problem:&lt;/h3&gt;
&lt;p&gt;This did not work because the object_name can not be overwritten in any way after the URL is presigned. This means that every team submission
actually resulted a new version of the same file. Hence, we could not easily list all files and in addition we could not transfer any data
in the filename. In our case the logic was &lt;team_name&gt;&lt;em&gt;&lt;model_type&gt;&lt;/em&gt;&lt;time_stamp&gt;&lt;/p&gt;
&lt;h2&gt;Attempt 2: Setting the Bucket to have a Public Write policy&lt;/h2&gt;
&lt;p&gt;Although I do not like using a public policy, it was the only solution on the short time frame to have people upload models from any location.
This was actually a very easy change in the amazon gui. So we set this up and used the requests library to post the files to S3.&lt;/p&gt;
&lt;p&gt;The next problem occurred when we tried to download the uploaded objects from S3. From the documentation from AWS we learned
that a publicly uploaded object will actually be owned by an 'anonymous user' instead of the bucket owner. Hence the bucket owner
cannot download the object even if all rights have been assigned. I do not understand this behavior as i would expect a bucket owner
to have the option to automatically own any objects inserted. &lt;/p&gt;
&lt;p&gt;The workaround is documented &lt;a href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-object-change-anonymous-ownership/"&gt;here&lt;/a&gt;. 
We were able to follow these steps using boto3 and python:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get your canonical id so it can be assigned to the ACL on your object.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;List all objects in your bucket so you can see what teams submitted a model: &lt;code&gt;submissions = s3.list_objects(Bucket=BUCKET_NAME, Prefix='models')&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We put this in a dataframe to have all the functionality available like sort, extract team names etc.
&lt;code&gt;df_all_subs = pd.DataFrame(submissions['Contents'])&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We created a function that would pick one specific model from one team and download it to the current location.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_submission_from_team&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;team_subs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_nr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;     Download the actual submission from a team.&lt;/span&gt;
&lt;span class="sd"&gt;     Also make sure the object is owned by the bucket owner, and not the anonymous user&lt;/span&gt;

&lt;span class="sd"&gt;     team_subs: pd.Dataframe containing the Content from a Read Objects operation.&lt;/span&gt;
&lt;span class="sd"&gt;     model_nr: int indicating which version of the model you want to download (as in rows of the df)&lt;/span&gt;
&lt;span class="sd"&gt;     &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

     &lt;span class="c1"&gt;# get the object and url data&lt;/span&gt;
     &lt;span class="n"&gt;object_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;team_subs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Key&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;model_nr&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s3://BUCKET_NAME/&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;object_name&lt;/span&gt;


     &lt;span class="c1"&gt;# required to set permission right&lt;/span&gt;
     &lt;span class="n"&gt;stream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;popen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;aws s3api put-object-acl --bucket &amp;quot;BUCKET_NAME&amp;quot; --key &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;object_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot; --acl bucket-owner-full-control --no-sign-request&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
     &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;stream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;popen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;aws s3 cp &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;  --metadata-directive REPLACE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
     &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

     &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;s3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_object_acl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bucket&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;object_name&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Owner&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;canonicalid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;KeyError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Wrong owner ID, something went wrong resetting the Owner from public to dedicated&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

     &lt;span class="n"&gt;s3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;object_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;object_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;file download completed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;object_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.zip&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Result&lt;/h2&gt;
&lt;p&gt;This solution worked and was used during the workshop. We could have teams upload as many models as they wanted
while having an automated way to monitor submissions and download them for rating. However, it feels really unclean
and there probably is a better way to do it. I think the main point is that public access should be avoided and that
spending time to create an interface to have individual object-URLs generated on the fly is probably a better solution. However,
this would have taken time that was not available.&lt;/p&gt;
&lt;p&gt;Maybe in a future post i will generate such an interface. If done i will show the link below.&lt;/p&gt;</content><category term="aws"></category><category term="s3"></category><category term="workshop"></category><category term="cloud"></category><category term="storage"></category></entry><entry><title>AWS S3 Notes</title><link href="/aws-s3-notes.html" rel="alternate"></link><published>2019-11-03T12:00:00+01:00</published><updated>2019-11-03T19:30:00+01:00</updated><author><name>Geert Jongen</name></author><id>tag:None,2019-11-03:/aws-s3-notes.html</id><summary type="html">&lt;p&gt;This post contains my notes on AWS S3&lt;/p&gt;</summary><content type="html">&lt;h1&gt;S3 - Simple Storage Service&lt;/h1&gt;
&lt;p&gt;See below my notes regarding the S3 service from Amazon. Largely based on a great master class @cloudguru developed by Stephen Wilding. These notes will develop further as i start using S3.&lt;/p&gt;
&lt;p&gt;Amazon S3 - Basic Storage Option from Amazon
Alternatives: Azure Data Lake Storage, Google... &lt;/p&gt;
&lt;h3&gt;Everything I need to remember&lt;/h3&gt;
&lt;h2&gt;S3 Basics&lt;/h2&gt;
&lt;p&gt;S3 is an Object Storage - Data &amp;amp; Associated Metadata stored as objects. An object is stored with the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Key: Name of the object&lt;/li&gt;
&lt;li&gt;Value: The data being stored &lt;strong&gt;0-5TB&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Version ID: A string of data assigned to an object when versioning is enabled&lt;/li&gt;
&lt;li&gt;Metadata = Name-Value pairs containing useful info on object&lt;/li&gt;
&lt;li&gt;Subresources = additional resources specifically assigned to an object&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The building blocks are &lt;strong&gt;buckets&lt;/strong&gt; which contain the objects that need to be stored. Both are classed as &lt;strong&gt;resources&lt;/strong&gt; in AWS. Hence they can be reffered to by an Amazon Resource Name (ARN). Default is to create up to 100 buckets. Buckets must be created in a &lt;strong&gt;region&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Buckets have &lt;strong&gt;subresources&lt;/strong&gt; that basically define what the configuration is. A &lt;strong&gt;subresource&lt;/strong&gt; needs a &lt;strong&gt;resource&lt;/strong&gt; to exist. An example is an Access Control Information; policies to control access)&lt;/p&gt;
&lt;p&gt;A bucket has a &lt;strong&gt;universal namespace&lt;/strong&gt;. Hence the name has to be unique on a global scale. next, you can access it using the following styles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Virtual: http://&lt;strong&gt;bucket&lt;/strong&gt;.s3.amazonaws.com/&lt;/li&gt;
&lt;li&gt;Virtual: http://&lt;strong&gt;bucket&lt;/strong&gt;.s3-aws-region.amazonaws.com/&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;S3 has flat storage. This means that there are actually no directories. it just stores files. However, using prefixes you can simulate directories. These are UTF-8 encoded and can't be longer than 1024 bytes. And try to use DNS-safe names.&lt;/p&gt;
&lt;p&gt;Another way to categorise files is &lt;strong&gt;tagging&lt;/strong&gt; this applies a key-value pair to an object. project=myproject. Using these tags you can change access rights, lifecycle mgt and other things. You can add up to 10 tags to 1 object.&lt;/p&gt;
&lt;h2&gt;Interacting with S3&lt;/h2&gt;
&lt;p&gt;There are a number of default operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GET: Download/Read&lt;/li&gt;
&lt;li&gt;PUT: Upload/Write&lt;/li&gt;
&lt;li&gt;DELETE: Delete&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Other Notes&lt;/h1&gt;
&lt;h2&gt;Consistency Model&lt;/h2&gt;
&lt;p&gt;S3 offers read-after-write consistency of puts of new objects. You can only read after the object has been fully created. S3 provides eventual consistency for overwrite puts and updates and deletes. Hence for updates old data may be returned. This model provides low latency and high throughput. There is no object locking. &lt;/p&gt;
&lt;p&gt;S3 performs checksum checks and automatically repairs if needed. 11x9s durability and 99,99% availability.&lt;/p&gt;
&lt;h2&gt;Security&lt;/h2&gt;
&lt;p&gt;S3 objects are secured by default, which means that all objects are private. This means that only owners can read them. If you want to change create a policy. There are two types of policies. &lt;strong&gt;Resource Policies&lt;/strong&gt; are applied directly to a resource. &lt;strong&gt;User Policies&lt;/strong&gt; are applied directly to your user account in IAM.&lt;/p&gt;
&lt;h3&gt;Resource Policies&lt;/h3&gt;
&lt;h4&gt;1. Access Control Lists (legacy)&lt;/h4&gt;
&lt;p&gt;These grant basic read/write permissions on objects and buckets to AWS accounts and predefined groups using an XML schema. It consists of a &lt;strong&gt;list of grants&lt;/strong&gt; where a &lt;strong&gt;grantee&lt;/strong&gt; receives &lt;strong&gt;permission&lt;/strong&gt; access and the permissions. &lt;/p&gt;
&lt;h4&gt;2. Bucket Policies&lt;/h4&gt;
&lt;p&gt;Bucket policies grant permissions for the bucket and objects  within to AWS accounts and IAM Users. The last point is the major difference. IAM users only occur in bucket policies. These are expressed using JSON. These replace ACLs&lt;/p&gt;
&lt;h3&gt;User Policies&lt;/h3&gt;
&lt;p&gt;User policies are directly granted to users, groups or roles using AWS IAM. Json-format as well. The major difference with bucket policies is that they are granted to users and therefore cannot be granted to anonymous users. Also these cannot be applied to the root user. You set them in AWS IAM.&lt;/p&gt;
&lt;p&gt;In most cases the preferred method of granting access is either via a User Policy or a Bucket Policy because they more fine-grained than ACLs. 
Use ACLs to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;manage access to objects not owned by the bucket owner&lt;/li&gt;
&lt;li&gt;manage access to individual objects when permissions must vary between objects&lt;/li&gt;
&lt;li&gt;grant permissions to the S3 Log Delivery Group on a bucket
Use Bucket Policies to&lt;/li&gt;
&lt;li&gt;Grant cross account access permissions that cannot be granted through an ACL.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To give an account access to see S3 buckets we had to assign s3:ListAllMyBuckets &amp;amp; s3:GetBucketLocation.&lt;/p&gt;
&lt;h3&gt;Policy Setup (Resource &amp;amp; User)&lt;/h3&gt;
&lt;p&gt;A policy contains the following concepts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Buckets I C PEARS and Users C EARS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;1. Principal: The account or user that is allowed access to the actions and resources specified in the statement&lt;/h4&gt;
&lt;p&gt;Only applied in a bucket policy and is specified as a name/value pair in either of these formats:
1.  &lt;code&gt;"AWS": "account-ARN"&lt;/code&gt;
2.  &lt;code&gt;arn:partition:service:region:namespace:relative-id&lt;/code&gt;
3.  &lt;code&gt;"Principal":{"AWS": "arn:aws:iam::123456789012:root"}&lt;/code&gt; (root-level access)
4.  &lt;code&gt;"Principal":{"AWS": "arn:aws:iam::123456789012:user/Steve"}&lt;/code&gt; (User-level access)
5.  &lt;code&gt;"Principal":"*"&lt;/code&gt; (Anonymous access)
3.  &lt;code&gt;"CanonicalUser":"64-digit-numerical-value"&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;2. Effect: The effect taken when the user requests the action. Either Allow or Deny.&lt;/h4&gt;
&lt;h4&gt;3. Conditions allow you to add 'if' like effect. Examples are:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Allow a user to put an object but only if it is uploaded with the public-read acl&lt;/li&gt;
&lt;li&gt;Allow a user to put an object but only if it is uploaded with S3 encryption enabled&lt;/li&gt;
&lt;li&gt;Allow a user to delete an objct if logged in using MFA&lt;/li&gt;
&lt;li&gt;Allow a user to create a bucket in a specific region&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consists of condition "StringEquals" and key-value pair. Example:
"Condition": {
    "StringEquals": {
    "s3:x-amz-acl": ["public-read"]
    }
}&lt;/p&gt;
&lt;h4&gt;4. Action: The list of permissions to allow or deny&lt;/h4&gt;
&lt;p&gt;there are 48 actions available currently. Very-finegrained&lt;/p&gt;
&lt;h4&gt;5. Resource: The bucket or object for which the access is applied. Specified as the ARN&lt;/h4&gt;
&lt;p&gt;In s3 it always looks the same:
1. &lt;code&gt;arn:aws:s3:::bucket_name&lt;/code&gt; (bucket-level)
2. &lt;code&gt;arn:aws:s3:::bucket_name/key_name&lt;/code&gt; (object-level)&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;arn:aws:s3:::steve_bucket&lt;/code&gt; (allow access to steve bucket)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;arn:aws:s3:::steve_bucket/*&lt;/code&gt; (allow access to all objects in steve bucket)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;arn:aws:s3:::steve_bucket/bucket/example.jpg&lt;/code&gt; (allow access to specific object)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;7. SID generally use as a description. (Not required by S3)&lt;/h4&gt;
&lt;p&gt;User policies do not have a principal in the statement because the principal is the user whom executes the policy.&lt;/p&gt;
&lt;h2&gt;Timed URL&lt;/h2&gt;
&lt;p&gt;if you only want to download/upload stuff based on conditions such as operation, bucket and time expiration. This has to be done programmatically.&lt;/p&gt;
&lt;p&gt;Python script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;boto&lt;/span&gt;
&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boto&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect_s3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate_url&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3600&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;#timeout (seconds)&lt;/span&gt;
                         &lt;span class="s1"&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;#operation&lt;/span&gt;
                         &lt;span class="n"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;bucket&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;object_key&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Example&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2019-12-31&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Sid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;AllowPutObjects&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;AWS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;arn:aws:iam::123456789012:user/Geert&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;s3:PutObject&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;arn:aws:s3:::my-bucket-name/*&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;       
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Charges&lt;/h2&gt;
&lt;p&gt;Charged for Storage
Charged for number of requests
Charged for Data Transfer Pricing (out)
Charged for Transfer Accelaration
Charged for Mgt functions:
1. Monitoring Metrics
2. Storage Class Analysis
3. S3 Inventory
Charged for Object Tagging&lt;/p&gt;
&lt;h2&gt;Storage Classes/Tiers&lt;/h2&gt;
&lt;p&gt;If you do not change class, you will get STANDARD.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th align="center"&gt;STANDARD&lt;/th&gt;
&lt;th align="center"&gt;REDUCED REDUNDANCY&lt;/th&gt;
&lt;th align="center"&gt;STANDARD_IA&lt;/th&gt;
&lt;th align="center"&gt;ONEZONE_IA&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Durability&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;td align="center"&gt;99.99%&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Availibity&lt;/td&gt;
&lt;td align="center"&gt;99.99%&lt;/td&gt;
&lt;td align="center"&gt;99.99%&lt;/td&gt;
&lt;td align="center"&gt;99.99%&lt;/td&gt;
&lt;td align="center"&gt;99.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min. Billable Obj Size&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;128KB&lt;/td&gt;
&lt;td align="center"&gt;128KB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min. Storage Duration&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;30 days&lt;/td&gt;
&lt;td align="center"&gt;30 days&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Retrieval Fee&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;per GB&lt;/td&gt;
&lt;td align="center"&gt;per GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Byte Latency&lt;/td&gt;
&lt;td align="center"&gt;ms&lt;/td&gt;
&lt;td align="center"&gt;ms&lt;/td&gt;
&lt;td align="center"&gt;ms&lt;/td&gt;
&lt;td align="center"&gt;ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Intelligent for unknown pattern, and glacier and deep archive for rarely accessed data&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th align="center"&gt;INTELLIGENT TIERING&lt;/th&gt;
&lt;th align="center"&gt;GLACIER&lt;/th&gt;
&lt;th align="center"&gt;DEEP_ARCHIVE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Durability&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;td align="center"&gt;99.999999999%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Availibity&lt;/td&gt;
&lt;td align="center"&gt;99.99%&lt;/td&gt;
&lt;td align="center"&gt;99.99%(restored objects)&lt;/td&gt;
&lt;td align="center"&gt;99.99%(restored objects)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min. Billable Obj Size&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min. Storage Duration&lt;/td&gt;
&lt;td align="center"&gt;30 days&lt;/td&gt;
&lt;td align="center"&gt;90 days&lt;/td&gt;
&lt;td align="center"&gt;180 days&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Retrieval Fee&lt;/td&gt;
&lt;td align="center"&gt;N/A&lt;/td&gt;
&lt;td align="center"&gt;per GB&lt;/td&gt;
&lt;td align="center"&gt;per GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Byte Latency&lt;/td&gt;
&lt;td align="center"&gt;ms&lt;/td&gt;
&lt;td align="center"&gt;select min/hrs&lt;/td&gt;
&lt;td align="center"&gt;hrs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;LIFECYCLE_POLICIES:&lt;/h3&gt;
&lt;p&gt;A set of rules   that S3 applies to a group of objects. A whole bucket or group within. You define rules (move from STANDARD &amp;gt; STANDARD_IA after 60 days, or expiration after 180 days)&lt;/p&gt;
&lt;h3&gt;Cross Account Access&lt;/h3&gt;
&lt;p&gt;Cross account users is if one AWS account A owns the bucket and grants access to antoher AWS account B. next that account B creates a user C that is given permission to use the bucket. This can happen and will affect billing &amp;amp; security.&lt;/p&gt;
&lt;h3&gt;Security Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;If the user and the bucket belong to the same account then all policies are evaluated at the same time.&lt;/strong&gt; If one policy allows and other is deny. Then the deny always wins. Also, if there is no allow, then the default is executed which is deny.
If not, (cross account) then the users account must first grant access to the user using the user policy. Next, the bucket owner must grant them access as well.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Owner is Bucket Owner" src="/images/AccessPolicyOwner.png"&gt;&lt;/p&gt;
&lt;p&gt;For object operations there is a small difference that the users account must grant access(1), then the bucket owner must NOT deny access(2) and then the object owner must grant access(3).
&lt;img alt="Bucket Owner is not User" src="/images/AccessPolicyNotOwner.png"&gt;&lt;/p&gt;
&lt;h2&gt;ACL Access Control List&lt;/h2&gt;
&lt;p&gt;Access Control Lists are XML Resource Policies used to grant access on both buckets and objects. Each bucket and object has an ACL attached to it as a sub-resource that defines permissions. The default ACL grants the resource owner full control over the object. Only an ACL is by default exist. No other policies.&lt;/p&gt;
&lt;p&gt;ACLs can only grant permissions to AWS accounts and pre-defined groups. You cannot give permissions to individual users.
Can only grant basic read/write access. So no conditional access and cannot deny. The XML contains a &lt;Owner&gt; section of the resource. and a Grant section where a &lt;Grantee&gt; ID and &lt;Permission&gt; is listed. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Permission&lt;/th&gt;
&lt;th align="center"&gt;When Granted on Bucket&lt;/th&gt;
&lt;th align="center"&gt;When Granted on Object&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;READ&lt;/td&gt;
&lt;td align="center"&gt;Allows Grantee to list objects in the bucket&lt;/td&gt;
&lt;td align="center"&gt;Allows Grantee to read the object and its metadata&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;WRITE&lt;/td&gt;
&lt;td align="center"&gt;Allows the Grantee to create, overwrite and delete any objects in the bucket&lt;/td&gt;
&lt;td align="center"&gt;Not Applicable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;READ_ACP&lt;/td&gt;
&lt;td align="center"&gt;Allows the grantee to read the bucket ACL&lt;/td&gt;
&lt;td align="center"&gt;Allows the grantee to read the object ACL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;WRITE_ACP&lt;/td&gt;
&lt;td align="center"&gt;Allows the grantee to write the ACL for bucket&lt;/td&gt;
&lt;td align="center"&gt;Allows the Grantee to write the ACL for the object&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;FULL_CONTROL&lt;/td&gt;
&lt;td align="center"&gt;Allows the Grantee to READ, WRITE, READ_ACP, WRITE_ACP permissions on the bucket.&lt;/td&gt;
&lt;td align="center"&gt;Allows the grantee to READ, READ_ACP, WRITE_ACP on the object&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;ACL to Groups&lt;/h3&gt;
&lt;p&gt;If ACL is applied to group, replace the canonical ID of user, with group URI. The following groups are available:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Group&lt;/th&gt;
&lt;th align="center"&gt;Description&lt;/th&gt;
&lt;th align="center"&gt;URI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Authenticated Users&lt;/td&gt;
&lt;td align="center"&gt;all AWS accounts&lt;/td&gt;
&lt;td align="center"&gt;http://acs.amazonaws.com/groups/global/AuthenticatedUsers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;All Users&lt;/td&gt;
&lt;td align="center"&gt;EVERYONE, public access&lt;/td&gt;
&lt;td align="center"&gt;http://acs.amazonaws.com/groups/global/AllUsers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Log Delivery&lt;/td&gt;
&lt;td align="center"&gt;Granting WRITE on bucket allows the group to write server access logs. Used for S3 bucket logging&lt;/td&gt;
&lt;td align="center"&gt;http://acs.amazonaws.com/groups/s3/LogDelivery&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Canned ACLs&lt;/h3&gt;
&lt;p&gt;These are predefined ACLs for common use cases&lt;/p&gt;
&lt;p&gt;In command line often used like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;aws s3 cp me.jpg s3://bucket-name/images/public/ --acl public-read&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aws s3 cp me.jpg s3://bucket-name/folder/ --acl public-read --profile my_account --region eu-west-1&lt;/code&gt;
this uses the &lt;code&gt;aws&lt;/code&gt; cmd line tool, &lt;code&gt;s3&lt;/code&gt; service, cp me.jpg to a bucket using the canned acl public-read, using my profile and in the region eu-west-1&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aws s3 cp me.jpg s3://bucket-name/public --acl authenticated-read --profile my_account --region eu-west-1&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Canned ACL&lt;/th&gt;
&lt;th align="center"&gt;Applies To&lt;/th&gt;
&lt;th align="left"&gt;Permission Granted&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;private&lt;/td&gt;
&lt;td align="center"&gt;bucket&amp;amp;object&lt;/td&gt;
&lt;td align="left"&gt;Owner gets FULL_CONTROL. No access to anyone else&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;public-read&lt;/td&gt;
&lt;td align="center"&gt;bucket&amp;amp;object&lt;/td&gt;
&lt;td align="left"&gt;Owner gets FULL_CONTROL, the ALLUSERS group gets READ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;public-read-write&lt;/td&gt;
&lt;td align="center"&gt;bucket&amp;amp;object&lt;/td&gt;
&lt;td align="left"&gt;Owner gets FULL_CONTROL. The AllUsers group gets READ&amp;amp;WRITE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;aws-exec-read&lt;/td&gt;
&lt;td align="center"&gt;bucket&amp;amp;object&lt;/td&gt;
&lt;td align="left"&gt;Owner gets FULL_CONTROL. Amazon EC2 gets READ access to GET an AMI for S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;authentication-read&lt;/td&gt;
&lt;td align="center"&gt;bucket&amp;amp;object&lt;/td&gt;
&lt;td align="left"&gt;Owner gets FULL_CONTROL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;bucket-owner-read&lt;/td&gt;
&lt;td align="center"&gt;Object&lt;/td&gt;
&lt;td align="left"&gt;Object Owner gets FULL_CONTROL The bucket owner gets READ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;bucket-owner-full-control&lt;/td&gt;
&lt;td align="center"&gt;object&lt;/td&gt;
&lt;td align="left"&gt;Object owner and bucket owner get FULL_CONTROL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;log-delivery-write&lt;/td&gt;
&lt;td align="center"&gt;Bucket&lt;/td&gt;
&lt;td align="left"&gt;The logdelivery group gets WRITE and READ_ACP access on the bucket&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="aws"></category><category term="s3"></category><category term="notes"></category><category term="cloud"></category><category term="storage"></category></entry></feed>